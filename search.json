[
  {
    "objectID": "posts/dreamy.html",
    "href": "posts/dreamy.html",
    "title": "Fluent dreaming for language models",
    "section": "",
    "text": "This is a companion page for our paper, “Fluent dreaming for language models.”.\nThere is an interactive demo of this page on Colab.\nDreaming is the process of maximizing some internal or output feature of a neural network by iteratively tweaking the input to the network. The most well-known example is DeepDream [1]. Besides making pretty images, dreaming is useful for interpreting the purpose of the internal components of a neural network [2]–[4]. To our knowledge, Dreaming has previously only been applied to vision models because the input space to a vision model is approximately continuous and algorithms like gradient descent work well. For language models, the input space is discrete and very different algorithms are needed. Extending work in the adversarial attacks literature [5], in the paper, we introduce the Evolutionary Prompt Optimization (EPO) algorithm for dreaming with language models.\nOn this page, we demonstrate running the EPO algorithm for a neuron in Phi-2. There is also a Colab notebook version of this page available."
  },
  {
    "objectID": "posts/dreamy.html#installation-and-setup",
    "href": "posts/dreamy.html#installation-and-setup",
    "title": "Fluent dreaming for language models",
    "section": "Installation and setup",
    "text": "Installation and setup\n\n\n\n\n\n\nClick to view install and imports\n\n\n\n\n\nFirst, we install necessary dependencies and install the dreamy library:\n\n!pip install \"poetry==1.7.1\" \"torch==2.1.2\" \"numpy==1.26.3\" \"transformers==4.37.0\" \"accelerate==0.26.1\" pandas pyarrow matplotlib ipywidgets\n![ -e dreamy_clone ] && rm -rf dreamy_clone\n!git clone https://github.com/Confirm-Solutions/dreamy dreamy_clone\n!cd dreamy_clone; poetry install\n\nNext, we import the dreamy library and load Phi-2:\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport transformers\nimport torch\nfrom IPython.display import HTML, display\n\nfrom dreamy.epo import epo, add_fwd_hooks, build_pareto_frontier\nfrom dreamy.attribution import resample_viz\n\n%config InlineBackend.figure_format='retina'\nnp.set_printoptions(edgeitems=10, linewidth=100)\npd.set_option(\"display.max_columns\", 100)\npd.set_option(\"display.max_colwidth\", None)\npd.set_option(\"display.max_rows\", 500)\n\n\n\n\nWe load up the Phi-2 model:\n\nmodel_name = \"microsoft/phi-2\"\nmodel = transformers.AutoModelForCausalLM.from_pretrained(\n    model_name,\n    low_cpu_mem_usage=True,\n    torch_dtype=\"auto\",\n    use_cache=False,\n    device_map=\"cuda\"\n)\ntokenizer = transformers.AutoTokenizer.from_pretrained(model_name)"
  },
  {
    "objectID": "posts/dreamy.html#running-epo",
    "href": "posts/dreamy.html#running-epo",
    "title": "Fluent dreaming for language models",
    "section": "Running EPO",
    "text": "Running EPO\nIn this section, we run the EPO algorithm. In order to use EPO, we first need to define an objective function. The objective function is responsible for executing a model forward pass and capturing whatever optimization “target” that we want to maximize. The API for defining an objective function is:\n\naccept an arbitrary set of arguments that will be passed on to the model.\nreturn a dictionary with a minimum of two keys:\n\ntarget: a target scalar that will be maximized.\nlogits: the token probabilities output by the model. These are used to calculate cross-entropy/fluency.\nother keys in the dictionary can optionally be used to pass info to a per-iteration monitoring callback. For more details, see the docstring of the epo function.\n\n\nHere, we are going to define an objective that maximizes the activation of a chosen neuron in Phi-2. We use a hook on the MLP layer to capture the activations of the chosen neuron. We maximize the activation only on the last token of the sequence.\n\ndef neuron_runner(layer, neuron):\n    def f(*model_args, **model_kwargs):\n        out = {}\n\n        def get_target(module, input, output):\n            out[\"target\"] = input[0][:, -1, neuron]\n\n        with add_fwd_hooks(\n            [\n                (model.model.layers[layer].mlp.fc2, get_target),\n            ]\n        ):\n            out[\"logits\"] = model(*model_args, **model_kwargs).logits\n        return out\n\n    return f\n\n\nrunner = neuron_runner(layer=8, neuron=1)\nhistory = epo(runner, model, tokenizer)\n\nbeginning step 299, current pareto frontier prompts:\npenalty=0.01 xentropy=8.09 target=4.56 ' study found another pattern by told-Mike Heyya, making[ the]'\npenalty=0.16 xentropy=7.59 target=4.48 ' study found another pattern by told-Mike Heyde, making[ the]'\npenalty=0.41 xentropy=5.16 target=3.50 ' study encountered another example of similar-unmatched pairs, with[ the]'\npenalty=0.98 xentropy=4.43 target=2.80 ' study found another pattern of similar-unmatched pairs, with[ the]'\npenalty=2.25 xentropy=4.43 target=2.80 ' study found another pattern of similar-unmatched pairs, with[ the]'"
  },
  {
    "objectID": "posts/dreamy.html#the-pareto-frontier",
    "href": "posts/dreamy.html#the-pareto-frontier",
    "title": "Fluent dreaming for language models",
    "section": "The Pareto frontier",
    "text": "The Pareto frontier\nTo visualize the results of this EPO run, we first plot the Pareto frontier of cross-entropy against activation.\n\npareto = build_pareto_frontier(tokenizer, history)\n\nordering = np.argsort(pareto.xentropy)\nplt.scatter(pareto.xentropy, pareto.target, c='k', label='Pareto frontier')\nfor i, k in enumerate(ordering):\n    plt.text(pareto.xentropy[k] + 0.05, pareto.target[k] + 0.05, pareto.text[k], fontsize=8, rotation=-25, va='top', color='black', alpha=1.0)\nplt.xlim(4, 11)\nplt.ylim(0, 5)\nplt.xlabel('Cross-entropy')\nplt.ylabel('Activation')\nplt.show()\n\n\n\n\n\n\n\n\nWe also plot the evolution of the Pareto frontier over the course of the optimization run.\n\nlinestyles = ['k--o', 'k:o', 'k--*', 'k:*']\nfor i, n in enumerate([20, 40, 100, 300]):\n    pareto = build_pareto_frontier(tokenizer, history.subset(slice(0, n)))\n    ordering = np.argsort(pareto.xentropy)\n    plt.plot(pareto.full_xentropy, pareto.full_target, linestyles[i % len(linestyles)], label=f\"{n} iterations\")\nplt.xlabel('Cross-entropy')\nplt.ylabel('Activation')\nplt.xlim([4, 12])\nplt.ylim([-0.25, 5])\nplt.legend(loc='lower right')\nplt.show()"
  },
  {
    "objectID": "posts/dreamy.html#thresholding-cross-entropy",
    "href": "posts/dreamy.html#thresholding-cross-entropy",
    "title": "Fluent dreaming for language models",
    "section": "Thresholding cross-entropy",
    "text": "Thresholding cross-entropy\nAn alternative way of visualizing the results of an EPO run is to consider only the subset of prompts with cross-entropy below some fixed threshold. Below, we plot the maximum activation across the 300 iterations of EPO for six different thresholds. The title of each plot shows the maximum activating prompt under the cross-entropy threshold across all iterations. The sharp drops every 50 iterations are from restarts. Sometimes there’s a plateau before the restart and other times progress is continuing. This suggests that a more adaptive restarting algorithm would perform better.\n\nplt.figure(figsize=(8, 12), constrained_layout=True)\nfor i, thresh in enumerate([5, 6, 7, 8, 9, 15]):\n    plt.subplot(3, 2, i + 1)\n    best_under = np.where(history.xentropy &lt; thresh, history.target, 0).max(axis=-1)\n    plt.plot(best_under, 'k-')\n    if i &gt;= 3:\n        plt.xlabel(\"Iteration\")\n    plt.ylabel(\"Max activation\")\n    plt.ylim(-0.1, 5)\n    if thresh &gt; 14:\n        plt.text(0.05, 0.95, \"No cross-entropy filter\", transform=plt.gca().transAxes, va=\"center\")\n    else:\n        plt.text(0.05, 0.95, f\"Cross-entropy &lt; {thresh}\", transform=plt.gca().transAxes, va=\"center\")\n\n    flat_xe = history.xentropy.flatten()\n    flat_target = history.target.flatten()\n    best_idx = np.where(flat_xe &lt; thresh, flat_target, 0).argmax()\n    best_ids = history.ids.reshape((-1, history.ids.shape[-1]))[best_idx]\n    best_text = tokenizer.decode(best_ids)\n    plt.title('\"' + best_text + '\"', fontsize=7)\nplt.show()"
  },
  {
    "objectID": "posts/dreamy.html#causal-token-attribution",
    "href": "posts/dreamy.html#causal-token-attribution",
    "title": "Fluent dreaming for language models",
    "section": "Causal token attribution",
    "text": "Causal token attribution\nThe visualizations below show the sensitivity to each token in the prompts. We first filter to the 32 “best” alternative tokens based on backpropagated token gradients. Then, amongst those 32 tokens, we calculate two sensitivities:\n\nthe drop in activation from swapping the token to the next highest activation alternative token. In the visualization, we show this in the height of the token bars.\nthe drop in activation from swapping the token to the lowest activation alternative token. In the visualization, we show this with the color of the tokens. Darker reds indicate a larger drop in activation.\n\nThe visualizations are interactive. Hover over each token to see a tooltip with the top-3 highest activation alternative tokens and the single lowest alternative token.\nWe show attribution visualizations for each prompt on the Pareto frontier. For all the prompts, swapping the last token can reduce the neuron activation to zero. Swapping other token can reduces the activation much less. The comma in the second-to-last position is also important and often has no viable substitute which is indicated by its tall bar.\n\nfor i in range(len(ordering)):\n    _, viz_html = resample_viz(\n        model,\n        tokenizer,\n        runner,\n        torch.tensor(pareto.ids[ordering[i]]).to(model.device),\n        target_name=\"L8.N1 activation\",\n    )\n    display(HTML(viz_html))\n\n\n    \n    \n    L8.N1 activation: 2.45           Cross-entropy: 4.42\n\n    \n    \n     study\n        Worst: '&lt;|endoftext|&gt;', 0.777\n        Top-3: (' surprise', 2.711),\n            ('Health', 2.654),\n            (' highlights', 2.639)\n        \n        \n         provided\n        Worst: ' past', 1.745\n        Top-3: (' take', 3.113),\n            (' seen', 3.088),\n            (' takes', 3.043)\n        \n        \n         another\n        Worst: ' storing', 0.721\n        Top-3: ('The', 2.707),\n            (' The', 2.664),\n            ('Take', 2.650)\n        \n        \n         example\n        Worst: 'If', 0.704\n        Top-3: (' fascinating', 2.355),\n            (' highlights', 2.025),\n            (' highlight', 1.979)\n        \n        \n         of\n        Worst: ' decreases', 1.067\n        Top-3: ('uing', 2.213),\n            ('are', 2.111),\n            ('ining', 2.070)\n        \n        \n         similar\n        Worst: '((', 0.290\n        Top-3: (' is', 3.100),\n            (' In', 2.729),\n            (' inconsistent', 2.721)\n        \n        \n        -\n        Worst: '99', 2.152\n        Top-3: ('iz', 2.635),\n            ('ores', 2.619),\n            ('ok', 2.611)\n        \n        \n        un\n        Worst: ' Despite', 1.612\n        Top-3: (' Maybe', 2.947),\n            ('Almost', 2.727),\n            (' Perhaps', 2.709)\n        \n        \n        matched\n        Worst: ' Island', 1.874\n        Top-3: (' inverted', 2.672),\n            (' lucky', 2.615),\n            (' shuffle', 2.586)\n        \n        \n         pairs\n        Worst: ' and', 0.147\n        Top-3: ('For', 2.793),\n            ('r', 2.750),\n            ('get', 2.748)\n        \n        \n        ,\n        Worst: ' f', 0.000\n        Top-3: ('�', 1.866),\n            ('2', 1.574),\n            ('to', 1.567)\n        \n        \n         with\n        Worst: '&lt;|endoftext|&gt;', 0.000\n        Top-3: (' potentially', 2.424),\n            (' without', 2.160),\n            (' accompanied', 1.850)\n        \n        \n        \n\n\n\n    \n    \n    L8.N1 activation: 2.80           Cross-entropy: 4.44\n\n    \n    \n     study\n        Worst: 'If', 0.960\n        Top-3: (' remember', 2.883),\n            (' potion', 2.854),\n            (' journey', 2.840)\n        \n        \n         found\n        Worst: ' perfect', 0.740\n        Top-3: (' encountered', 3.115),\n            (' noticed', 3.016),\n            (' highlighted', 3.008)\n        \n        \n         another\n        Worst: ''t', 0.618\n        Top-3: (' interesting', 2.617),\n            (' fascinating', 2.607),\n            ('known', 2.445)\n        \n        \n         pattern\n        Worst: ' where', 0.757\n        Top-3: (' take', 2.508),\n            (' like', 2.385),\n            (' this', 2.213)\n        \n        \n         of\n        Worst: ' Even', 1.514\n        Top-3: (' I', 3.070),\n            (' i', 2.977),\n            (' 1', 2.893)\n        \n        \n         similar\n        Worst: '-(', 1.491\n        Top-3: (' Like', 3.100),\n            (' adjustable', 3.012),\n            (' Takes', 3.008)\n        \n        \n        -\n        Worst: '&lt;|endoftext|&gt;', 1.514\n        Top-3: ('True', 2.936),\n            ('ille', 2.934),\n            ('iz', 2.852)\n        \n        \n        un\n        Worst: ' Despite', 1.522\n        Top-3: ('That', 3.006),\n            (' maybe', 2.900),\n            (']]', 2.895)\n        \n        \n        matched\n        Worst: ' both', 2.188\n        Top-3: ('-', 2.809),\n            ('jit', 2.785),\n            ('action', 2.768)\n        \n        \n         pairs\n        Worst: ' Random', 1.240\n        Top-3: ('num', 3.160),\n            ('FIL', 3.084),\n            ('bool', 3.072)\n        \n        \n        ,\n        Worst: ' System', 0.000\n        Top-3: ('()', 2.291),\n            ('names', 2.018),\n            ('//', 1.967)\n        \n        \n         with\n        Worst: ' There', 0.000\n        Top-3: (' as', 2.463),\n            (' enough', 2.244),\n            (' so', 2.195)\n        \n        \n        \n\n\n\n    \n    \n    L8.N1 activation: 2.80           Cross-entropy: 4.44\n\n    \n    \n     study\n        Worst: 'If', 0.960\n        Top-3: (' remember', 2.883),\n            (' potion', 2.854),\n            (' journey', 2.840)\n        \n        \n         found\n        Worst: ' perfect', 0.740\n        Top-3: (' encountered', 3.115),\n            (' noticed', 3.016),\n            (' highlighted', 3.008)\n        \n        \n         another\n        Worst: ''t', 0.618\n        Top-3: (' interesting', 2.617),\n            (' fascinating', 2.607),\n            ('known', 2.445)\n        \n        \n         pattern\n        Worst: ' where', 0.757\n        Top-3: (' take', 2.508),\n            (' like', 2.385),\n            (' this', 2.213)\n        \n        \n         of\n        Worst: ' Even', 1.514\n        Top-3: (' I', 3.070),\n            (' i', 2.977),\n            (' 1', 2.893)\n        \n        \n         similar\n        Worst: '-(', 1.491\n        Top-3: (' Like', 3.100),\n            (' adjustable', 3.012),\n            (' Takes', 3.008)\n        \n        \n        -\n        Worst: '&lt;|endoftext|&gt;', 1.514\n        Top-3: ('True', 2.936),\n            ('ille', 2.934),\n            ('iz', 2.852)\n        \n        \n        un\n        Worst: ' Despite', 1.522\n        Top-3: ('That', 3.006),\n            (' maybe', 2.900),\n            (']]', 2.895)\n        \n        \n        matched\n        Worst: ' both', 2.188\n        Top-3: ('-', 2.809),\n            ('jit', 2.785),\n            ('action', 2.768)\n        \n        \n         pairs\n        Worst: ' Random', 1.240\n        Top-3: ('num', 3.160),\n            ('FIL', 3.084),\n            ('bool', 3.072)\n        \n        \n        ,\n        Worst: ' System', 0.000\n        Top-3: ('()', 2.291),\n            ('names', 2.018),\n            ('//', 1.967)\n        \n        \n         with\n        Worst: ' There', 0.000\n        Top-3: (' as', 2.463),\n            (' enough', 2.244),\n            (' so', 2.195)\n        \n        \n        \n\n\n\n    \n    \n    L8.N1 activation: 2.98           Cross-entropy: 4.60\n\n    \n    \n     study\n        Worst: ' ensure', 1.014\n        Top-3: (' remember', 3.246),\n            (' journey', 3.178),\n            (' outing', 3.160)\n        \n        \n         found\n        Worst: 'Represent', 1.725\n        Top-3: (' encountered', 3.502),\n            (' noticed', 3.170),\n            (' encounter', 3.094)\n        \n        \n         another\n        Worst: ' getting', 0.897\n        Top-3: (' out', 2.584),\n            (' take', 2.510),\n            ('our', 2.428)\n        \n        \n         example\n        Worst: 'If', 1.215\n        Top-3: (' this', 2.213),\n            (' is', 2.172),\n            (' my', 2.145)\n        \n        \n         of\n        Worst: '((', 0.761\n        Top-3: (''m', 3.395),\n            (' called', 3.254),\n            ('�', 2.783)\n        \n        \n         similar\n        Worst: '(\"', 1.018\n        Top-3: (' After', 3.141),\n            (' like', 3.123),\n            (' In', 3.109)\n        \n        \n        -\n        Worst: '&lt;|endoftext|&gt;', 1.471\n        Top-3: ('ille', 3.125),\n            ('iz', 3.086),\n            ('�', 3.068)\n        \n        \n        un\n        Worst: ' devices', 2.066\n        Top-3: (' Maybe', 3.170),\n            (' maybe', 3.119),\n            (' liked', 3.076)\n        \n        \n        matched\n        Worst: ' ozone', 2.521\n        Top-3: (' inverted', 3.166),\n            (' decision', 3.037),\n            (' incidence', 2.996)\n        \n        \n         pairs\n        Worst: ' False', 1.593\n        Top-3: ('num', 3.480),\n            ('match', 3.439),\n            ('design', 3.357)\n        \n        \n        ,\n        Worst: ' jaw', 0.000\n        Top-3: (' here', 2.363),\n            (' however', 2.311),\n            (' but', 2.104)\n        \n        \n         with\n        Worst: '&lt;|endoftext|&gt;', 0.000\n        Top-3: (' although', 2.957),\n            (' but', 2.484),\n            (' additionally', 2.473)\n        \n        \n        \n\n\n\n    \n    \n    L8.N1 activation: 3.50           Cross-entropy: 5.15\n\n    \n    \n     study\n        Worst: ' ensure', 1.600\n        Top-3: (' observation', 3.592),\n            (' town', 3.572),\n            (' remember', 3.551)\n        \n        \n         encountered\n        Worst: ' Across', 1.575\n        Top-3: ('rov', 2.795),\n            ('rett', 2.773),\n            (' counters', 2.736)\n        \n        \n         another\n        Worst: ' make', 2.018\n        Top-3: (' John', 3.430),\n            (' take', 3.365),\n            (' town', 3.307)\n        \n        \n         example\n        Worst: 'If', 1.681\n        Top-3: (' phenomenon', 3.490),\n            (' fascinating', 3.307),\n            (' episode', 2.926)\n        \n        \n         of\n        Worst: '?\"', 1.208\n        Top-3: (''m', 3.816),\n            ('ivating', 3.238),\n            ('--', 3.035)\n        \n        \n         similar\n        Worst: '(\"', 1.020\n        Top-3: (' is', 3.891),\n            (' -', 3.814),\n            (' are', 3.658)\n        \n        \n        -\n        Worst: '�', 3.246\n        Top-3: ('ille', 3.617),\n            ('iz', 3.602),\n            ('et', 3.598)\n        \n        \n        un\n        Worst: ' Despite', 1.779\n        Top-3: (' blind', 3.664),\n            (' blending', 3.594),\n            (' match', 3.582)\n        \n        \n        matched\n        Worst: 'ás', 3.066\n        Top-3: (' inverted', 3.633),\n            (' shuffle', 3.617),\n            (' matching', 3.568)\n        \n        \n         pairs\n        Worst: ' motivated', 1.583\n        Top-3: ('match', 3.795),\n            ('num', 3.760),\n            ('haus', 3.729)\n        \n        \n        ,\n        Worst: '&lt;|endoftext|&gt;', 0.023\n        Top-3: (' here', 2.568),\n            ('ames', 2.469),\n            ('ets', 2.428)\n        \n        \n         with\n        Worst: '&lt;|endoftext|&gt;', 0.000\n        Top-3: (' relying', 3.643),\n            (' including', 2.344),\n            (' especially', 2.332)\n        \n        \n        \n\n\n\n    \n    \n    L8.N1 activation: 4.48           Cross-entropy: 7.59\n\n    \n    \n     study\n        Worst: '_', 2.846\n        Top-3: (' involvement', 4.547),\n            (' development', 4.488),\n            (' Data', 4.281)\n        \n        \n         found\n        Worst: 'agraph', 1.615\n        Top-3: (' noticed', 4.297),\n            (' seen', 4.172),\n            (' highlighted', 4.133)\n        \n        \n         another\n        Worst: ' If', 0.915\n        Top-3: (' strange', 3.670),\n            (' clear', 3.377),\n            (' was', 3.314)\n        \n        \n         pattern\n        Worst: ';', 0.543\n        Top-3: (' seen', 3.510),\n            (' use', 3.502),\n            (' take', 3.467)\n        \n        \n         by\n        Worst: 'agu', 3.014\n        Top-3: (' during', 4.383),\n            (' at', 4.371),\n            (' using', 4.316)\n        \n        \n         told\n        Worst: 'Answer', 3.424\n        Top-3: (' attributed', 4.391),\n            (' received', 4.387),\n            (' noticed', 4.383)\n        \n        \n        -\n        Worst: ' considerations', 2.334\n        Top-3: (' attract', 4.277),\n            (' borrow', 4.227),\n            (' attracted', 4.199)\n        \n        \n        Mike\n        Worst: ' losses', 2.174\n        Top-3: ('ixie', 4.395),\n            ('ogo', 4.359),\n            ('ony', 4.336)\n        \n        \n         Hey\n        Worst: '\"?', 2.256\n        Top-3: ('ention', 4.238),\n            ('onto', 4.230),\n            ('weet', 4.219)\n        \n        \n        de\n        Worst: ' cannibal', 3.965\n        Top-3: ('gil', 4.621),\n            ('laus', 4.605),\n            ('endi', 4.594)\n        \n        \n        ,\n        Worst: ' decision', 0.113\n        Top-3: ('Ev', 3.006),\n            ('vert', 3.000),\n            ('ipation', 2.850)\n        \n        \n         making\n        Worst: ' How', 0.000\n        Top-3: (' without', 3.291),\n            (' because', 3.133),\n            (' to', 3.070)\n        \n        \n        \n\n\n\n    \n    \n    L8.N1 activation: 4.75           Cross-entropy: 8.69\n\n    \n    \n     findings\n        Worst: ' Unlike', 2.154\n        Top-3: ('bl', 4.746),\n            ('oys', 4.688),\n            ('isk', 4.676)\n        \n        \n         noted\n        Worst: 'Neither', 1.698\n        Top-3: (' notices', 4.719),\n            (' noticed', 4.699),\n            (' spotted', 4.562)\n        \n        \n         another\n        Worst: ' which', 1.312\n        Top-3: (' two', 3.750),\n            ('A', 3.641),\n            ('a', 3.512)\n        \n        \n         paradox\n        Worst: ' never', 2.424\n        Top-3: (' case', 4.195),\n            (' one', 3.656),\n            (' taking', 3.598)\n        \n        \n        ...\n        Worst: ';', 2.314\n        Top-3: (' –', 4.234),\n            (' -', 4.211),\n            (' —', 4.203)\n        \n        \n        for\n        Worst: ' who', 1.547\n        Top-3: (' off', 4.395),\n            (' #', 4.266),\n            (' by', 4.266)\n        \n        \n        _\n        Worst: ' Within', 3.043\n        Top-3: (' Rob', 3.996),\n            (' size', 3.986),\n            (' recalls', 3.986)\n        \n        \n        x\n        Worst: ' Human', 3.998\n        Top-3: ('May', 4.801),\n            ('Bi', 4.742),\n            ('Co', 4.711)\n        \n        \n        Blake\n        Worst: ' flooded', 3.434\n        Top-3: (' Aval', 4.754),\n            (' Mish', 4.742),\n            (' Rip', 4.715)\n        \n        \n        ,\n        Worst: ' Although', 0.357\n        Top-3: (' *', 4.410),\n            (' +', 4.340),\n            ('..', 4.312)\n        \n        \n        ,\n        Worst: ' However', 0.227\n        Top-3: (' heard', 2.730),\n            (' enjoyed', 2.502),\n            (' basically', 2.400)\n        \n        \n         although\n        Worst: '-', 0.000\n        Top-3: (' as', 3.809),\n            (' to', 3.791),\n            (' if', 3.525)\n        \n        \n        \n\n\n\n    \n    \n    L8.N1 activation: 4.80           Cross-entropy: 9.49\n\n    \n    \n     findings\n        Worst: 'Get', 3.012\n        Top-3: (' Wan', 4.387),\n            (' campaign', 4.379),\n            (' Mas', 4.324)\n        \n        \n         found\n        Worst: '?\"', 2.156\n        Top-3: (' remember', 4.449),\n            (' remembered', 4.395),\n            (' mentions', 4.262)\n        \n        \n         another\n        Worst: ' although', 0.634\n        Top-3: (' à', 3.934),\n            ('Next', 3.537),\n            (' Other', 3.529)\n        \n        \n         twist\n        Worst: ' cancel', 3.672\n        Top-3: (' kicker', 4.465),\n            (' funny', 4.387),\n            (' humour', 4.387)\n        \n        \n         see\n        Worst: ' Flight', 3.949\n        Top-3: (' football', 4.520),\n            (' tennis', 4.512),\n            ('OSS', 4.508)\n        \n        \n         study\n        Worst: '`,', 3.445\n        Top-3: ('*', 4.719),\n            (' 1950', 4.707),\n            (' ()', 4.641)\n        \n        \n        _\n        Worst: 'A', 0.000\n        Top-3: (' …', 3.555),\n            (' where', 1.688),\n            ('def', 0.069)\n        \n        \n        un\n        Worst: ' Load', 3.869\n        Top-3: ('Log', 4.906),\n            ('Answer', 4.902),\n            ('Hash', 4.887)\n        \n        \n        interrupted\n        Worst: ' Statistical', 4.020\n        Top-3: (' Principle', 4.848),\n            ('ABC', 4.824),\n            (' Hardy', 4.820)\n        \n        \n         aversion\n        Worst: ' cite', 3.535\n        Top-3: ('omsday', 4.637),\n            ('tymology', 4.621),\n            (' Argon', 4.605)\n        \n        \n         control\n        Worst: ' which', 1.398\n        Top-3: ('os', 4.711),\n            (' II', 4.711),\n            ('IS', 4.707)\n        \n        \n         although\n        Worst: '-', 0.000\n        Top-3: (' to', 3.316),\n            (' if', 3.266),\n            (' into', 3.166)\n        \n        \n        \n\n\n\n    \n    \n    L8.N1 activation: 4.82           Cross-entropy: 10.28\n\n    \n    \n     findings\n        Worst: 'Bob', 3.613\n        Top-3: (' Wan', 4.379),\n            (' Bond', 4.301),\n            (' team', 4.297)\n        \n        \n         found\n        Worst: '?\"', 1.916\n        Top-3: (' remember', 4.348),\n            (' remembered', 4.312),\n            (' showcases', 4.195)\n        \n        \n         another\n        Worst: ' ((', 0.625\n        Top-3: (' à', 3.912),\n            (' In', 3.451),\n            (' de', 3.373)\n        \n        \n         twist\n        Worst: ' slides', 3.492\n        Top-3: (' kicker', 4.449),\n            (' humour', 4.344),\n            (' funny', 4.301)\n        \n        \n         see\n        Worst: ' Star', 3.791\n        Top-3: ('�', 4.426),\n            ('alsa', 4.410),\n            ('apo', 4.406)\n        \n        \n         study\n        Worst: ','', 3.037\n        Top-3: ('*', 4.602),\n            (' 1950', 4.590),\n            (' online', 4.551)\n        \n        \n        _\n        Worst: 'A', 0.000\n        Top-3: (' from', 4.699),\n            (' do', 4.355),\n            (' called', 4.230)\n        \n        \n        done\n        Worst: '(', 2.484\n        Top-3: ('pped', 4.891),\n            (' by', 4.887),\n            ('aked', 4.867)\n        \n        \n        ivariate\n        Worst: ' Statistical', 3.707\n        Top-3: ('wik', 4.793),\n            (' graphical', 4.770),\n            ('iminary', 4.762)\n        \n        \n         aversion\n        Worst: ' Comment', 3.689\n        Top-3: ('pin', 4.621),\n            (' pH', 4.594),\n            (' inclusion', 4.586)\n        \n        \n         control\n        Worst: ' some', 0.629\n        Top-3: ('ining', 4.715),\n            ('ing', 4.691),\n            ('IS', 4.668)\n        \n        \n         although\n        Worst: '-', 0.000\n        Top-3: (' to', 3.254),\n            (' into', 3.117),\n            (' as', 3.107)\n        \n        \n        \n\n\n\n    \n    \n    L8.N1 activation: 4.84           Cross-entropy: 10.65\n\n    \n    \n     findings\n        Worst: 'Under', 3.744\n        Top-3: (' Roll', 4.477),\n            (' Wan', 4.387),\n            (' team', 4.367)\n        \n        \n         found\n        Worst: '?\"', 2.031\n        Top-3: (' remember', 4.387),\n            (' remembered', 4.363),\n            (' showcases', 4.227)\n        \n        \n         another\n        Worst: ' Although', 0.654\n        Top-3: (' An', 3.635),\n            ('Next', 3.506),\n            (' In', 3.492)\n        \n        \n         twist\n        Worst: ' slides', 3.438\n        Top-3: (' kicker', 4.453),\n            (' funny', 4.375),\n            (' humour', 4.359)\n        \n        \n         see\n        Worst: ' Sodium', 3.941\n        Top-3: ('apo', 4.535),\n            ('anna', 4.496),\n            ('uan', 4.492)\n        \n        \n         study\n        Worst: ';', 3.201\n        Top-3: ('*', 4.691),\n            (' found', 4.555),\n            (' software', 4.551)\n        \n        \n        _\n        Worst: 'A', 0.000\n        Top-3: (' did', 4.305),\n            (' where', 1.965),\n            ('def', 0.320)\n        \n        \n        ds\n        Worst: ' Galaxy', 4.102\n        Top-3: ('itivity', 4.832),\n            ('azz', 4.797),\n            ('istent', 4.797)\n        \n        \n        ivariate\n        Worst: '===', 4.176\n        Top-3: ('iminary', 4.828),\n            (' Nielsen', 4.766),\n            (' Draper', 4.758)\n        \n        \n         aversion\n        Worst: ' «', 2.484\n        Top-3: (' filming', 4.602),\n            ('reporting', 4.598),\n            (' pseudonym', 4.566)\n        \n        \n         control\n        Worst: ' some', 0.612\n        Top-3: ('ining', 4.668),\n            ('ing', 4.664),\n            ('IS', 4.617)\n        \n        \n         although\n        Worst: '-', 0.000\n        Top-3: (' to', 3.211),\n            (' into', 3.053),\n            (' as', 3.039)"
  },
  {
    "objectID": "posts/TDC2023.html",
    "href": "posts/TDC2023.html",
    "title": "Takeaways from the NeurIPS 2023 Trojan Detection Competition",
    "section": "",
    "text": "This post summarizes our team’s takeaways as participants in the NeurIPS 2023 Trojan Detection Competition. Since we won one of the competition tracks, we have been invited to contribute to the joint competition paper. Here, we share our personal opinions and takeaways from the event."
  },
  {
    "objectID": "posts/TDC2023.html#footnotes",
    "href": "posts/TDC2023.html#footnotes",
    "title": "Takeaways from the NeurIPS 2023 Trojan Detection Competition",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWhile an unintended trigger might exist in the base model, many unintended triggers are a side effect of the training process for inserting the intended triggers.↩︎\nThe “residual stream” at layer L↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Confirm",
    "section": "",
    "text": "Fluent dreaming for language models\n\n\nPaper companion page.\n\n\n\nT. Ben Thompson, Zygimantas Straznickas, Michael Sklar\n\n\nJan 23, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nTakeaways from the NeurIPS 2023 Trojan Detection Competition\n\n\nSummarizing our takeaways from TDC2023\n\n\n\nZygimantas Straznickas, T. Ben Thompson, Michael Sklar\n\n\nJan 13, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n6 Ways to Fight the Interpretability Illusion\n\n\nNotes on using optimization and causal models for interpretability.\n\n\n\nMichael Sklar\n\n\nNov 30, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nA catalog of several million tasks Pythia can do\n\n\n\n\n\n\nT. Ben Thompson, Michael Sklar\n\n\nJun 25, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/fight_the_illusion.html",
    "href": "posts/fight_the_illusion.html",
    "title": "6 Ways to Fight the Interpretability Illusion",
    "section": "",
    "text": "Recommended pre-reading:\n\nGeiger et al.’s DAS and Boundless DAS.\nAn Interpretability Illusion for Activation Patching of Arbitrary Subspaces.\nThe corresponding ICLR paper, “Is This the Subspace You Are Looking For?”\n\n\nThis post is motivated by Lange, Makelov, and Nanda’s LessWrong post Interpretability Illusion for Activation Patching and ICLR paper. They study Geiger et al’s DAS method, which uses optimization to identify an abstracted causal model with a small subset of dimensions in a neural network’s residual stream or internal MLP layer. Their results show that DAS can, depending on the situation, turn up both “correct” and “spurious” findings on the train-set. From the investigations in the ICLR paper and conversations with a few researchers, my understanding is these “spurious” directions have not performed well on held-out generalization sets, so in practice it is easy to distinguish the “illusions” from “real effects”. But, I am interested in developing even stronger optimize-to-interpret methods. With more powerful optimizers, illusion effects should be even stronger, and competition from spurious signals may make true signals harder to locate in training. So, here are 6 possible ways to fight against the interpretability illusion. Most of them can be tried in combination.\n\nThe causal model still holds, and may still be what we want.: We call it an interpretability illusion because we are failing to describe the model’s normal functioning. But unusual functioning is fine for some goals! Applications include:\n\nFinding latent circuits which might be targetable by optimized non-routine inputs. For example, these circuits might be used in an adversarial attack.\n“Pinning” a false belief into the model, for testing or alignment training. For example, forcing the model to believe it is not being watched, in order to test deception or escape behavior.\n\nThe key point is that the interpretability illusion is a failure to describe typical model operation, but a success for enacting the causal model.\nStudy more detailed causal models with multiple output streams, multiple options for the input variables, or more compositions. To start, notice that it is obviously good to have more outputs/consequences of the causal mode in the optimization. Why? First, if we have multiple output-measurements at the end of the causal graph, it is harder for a spurious direction to perform well on all of them by chance. Additionally, if an abstract causal model has modular pieces, then there should be exponentially many combinatorial-swap options that we can test. To score well on the optimization’s training-loss across all swaps (in the language of DAS this is a high “IIA”), the spurious structure would have to be very sophisticated. While Lange et al. show that spurious solutions may arise for searches in 1 direction, it should be less likely to occur for pairs of directions, and less likely yet for full spurious circuits. So, illusion problems may be reduced by scaling up the complexity of the causal model. Some possible issues remain, though:\n\nIn some cases we may struggle to identify specific directions within a multi-part model; i.e., we might find convincing overall performance for a circuit, but an individual dimension or two could be spurious, and we might be unable to determine exactly which.\nThis approach relies on big, deep, abstract causal models existing inside the networks, with sufficient robustness in their functioning across variable changes. There is some suggestive work on predictable / standardized structures in LLM’s, from investigations like Feng and Steinhardt (2023)’s entity binding case study, the indirect object identification (IOI) paper, and studies of recursive tasks. However, the consistency/robustness and DAS-discoverability of larger structures in scaled-up models is not yet clear. More case studies in larger models would be valuable.\n\nMeasure generalizability, and use it to filter out spurious findings after-the-fact. This is just common-sense, and researchers are already doing this in several ways. We can construct train/test splits with random sampling, and conclude a found direction is spurious if it does not generalize on the test data; or we could ask how the patched model generalizes out-of-training-distribution following a small perturbation, such as adding extra preceding tokens. Spurious solutions are likely to be sensitive to minor changes, and for many purposes we are primarily interested in causal models that generalize well. As mentioned earlier, the ICLR paper’s `spurious’ findings performed sufficiently poorly on generalization sets that they could easily be distinguished from real effects.\nQuantify a null distribution. In the “Illusion” post, Lange et al. show that the strength of the spurious signal depends on how many neurons it is allowed to optimize over. So, a very strong signal, taken over a small optimization set, should be more convincing. Thinking as statisticians, we could attempt to construct a null distribution for the spurious signals; this approach could offer evidence that a causal map element is being represented at all. We can do this inference for individual pieces of a larger causal model, with each component having its own uncertainty.\nUse unsupervised feature extraction as a first step. Recent interpretability work with auto-encoders (Bricken et al. 2023, Cunningham et al. 2023) suggests that many of a small transformer’s most important features can be identified. If this technique scales well, it could vastly reduce the amount of optimization pressure needed to identify the right directions, shrinking the search space and reducing optimistic bias and spurious findings.\nIncorporate additional information as a prior / penalty for optimization. As Lange et al. note in the “Illusion” post, and as described in Section 5 of the ICLR paper, it is possible to supply additional evidence that a found direction is faithful or not. In the case study with the IOI task, they argued the direction found by DAS on a residual layer fell within the query subspace of human-identified name mover heads. More generally, if intuitions about faithfulness can be scored with a quantitative metric, then tacking that metric onto the optimization as a penalty should help the optimizer favor correct directions over spurious solutions. Still, using this approach requires answering two difficult questions: what additional evidence to choose, and then how to quantify it? Some rough possibilities:\n\nIf we know of structures that should be related to the task, such as entity bindings (Feng and Steinhardt (2023)), we can try to build outwards from them; or if we have a reliable feature dictionary from sparse auto-encoders or “belief graph” per Hase et al. 2021 which offers advance predictions for how subsequent layers’ features may react to a change, we can penalize lack of correlation or causal effects on downstream features.\nSomehow use the basic structure of the network to quantify which directions are ‘dormant.’ Although this sounds simple, I am unsure how to do it, given the indeterminacy of what a ‘dormant’ direction even means (this issue is described in Lange et al.’s lesswrong post, Appendix section: the importance of correct model units.)\nPerhaps next-gen AI will offer accurate “auto-grading”, giving a general yet quantitative evaluation of plausibility of found solutions\n\nUsing extra information in this way unfortunately spends its usability for validation. But preventing the optimization from getting stuck on spurious signals may be the higher priority.\n\n\nThanks to Atticus Geiger, Jing Huang, Zhengxuan Wu, Ben Thompson, Zygimantas Straznickas and others for conversations and feedback on earlier drafts.\n\n\n\nCitationBibTeX citation:@online{sklar2023,\n  author = {Sklar, Michael},\n  title = {6 {Ways} to {Fight} the {Interpretability} {Illusion}},\n  date = {2023-11-30},\n  url = {https://confirmlabs.org/posts/fight_the_illusion.html},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nM.\nSklar, “6 Ways to Fight the Interpretability Illusion,”\nNov. 30, 2023. https://confirmlabs.org/posts/fight_the_illusion.html"
  },
  {
    "objectID": "posts/catalog.html",
    "href": "posts/catalog.html",
    "title": "A catalog of several million tasks Pythia can do",
    "section": "",
    "text": "We’re sharing datasets that we hope will be useful for language model interpretability."
  },
  {
    "objectID": "posts/catalog.html#the-data",
    "href": "posts/catalog.html#the-data",
    "title": "A catalog of several million tasks Pythia can do",
    "section": "The data",
    "text": "The data\nIn following sections we will give details on the construction and statistics of these datasets. Before continuing, we share some interactive data previews:\n\nDeletion: the first 25000 rows of pile_scan_4.\nBigrams: the entirety of pile_top_bigrams, which contains bigrams with suffix probability greater than 50%.\nTrigrams: the first 25000 rows of pile_top_trigrams, which contains trigrams with suffix probability greater than 50% and count greater than 1000.\n\n\nDeletionBigramsTrigrams\n\n\nThe columns of the table below:\n\ntext: the two prompts provided. The additional token of backwards context is surrounded by square brackets. The example in the introduction would be written \"[_chloride],_or_common_table\".\ntoken_short: the most likely next token predicted by Pythia-2.8B for the four token prompt.\ntoken_long: the most likely next token predicted by Pythia-2.8B for the five token prompt.\np_short: the probability Pythia-2.8B assigns to token_short.\np_long: the probability Pythia-2.8B assigns to token_long.\nJS: the Jensen-Shannon divergence between the model’s output distributions for the four and five token prompts.\n\nNote:\n\nin the table, spaces are replaced with underscores for clarity.\nthere are offensive tokens in the dataset. We have not removed them.\n\n\n\n\n\n\n\n\n\ntext\ntoken_short\ntoken_long\np_short\np_long\nJS\n\n\n\n\nLoading... (need help?)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe table below shows bigram completions in The Pile sorted by the frequency of occurence of the prefix token:\n\ntoken#: the tokens of the bigram.\nsum_count: the number of times the first token of the bigram occurs in The Pile.\nfrac_max: the fraction of first token appearances that are followed by the most common bigram completion. For example, 50.3% of the time the model sees \" need\", the correct next token is \" to\".\np_2.8b: the probability Pythia-2.8B assigns to the most likely completion token when prompted with just the prefix token.\n\nNote:\n\nin the table, spaces are replaced with underscores for clarity.\nthere are offensive tokens in the dataset. We have not removed them.\n\n\n\n\n\n\n\n\n\ntoken0\ntoken1\nsum_count\nfrac_max\np_2.8b\n\n\n\n\nLoading... (need help?)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe table below shows trigram completions in The Pile sorted by the frequency of occurence of the prefix bigram:\n\ntoken#: the tokens of the trigram.\nsum_count: the number of times the prefix bigram occurs in The Pile.\nfrac_max: the fraction of bigram appearances that are followed by the most common third token. For example, when prompted with the tokens [\"://\", \"www\"], 99.4% of the time, the next token is \".\".\np_2.8b: the probability Pythia-2.8B assigns to the most likely completion token when prompted with the prefix bigram.\n\nNote:\n\nin the table, spaces are replaced with underscores for clarity.\nthere are offensive tokens in the dataset. We have not removed them.\n\n\n\n\n\n\n\n\n\ntoken0\ntoken1\ntoken2\nsum_count\nfrac_max\np_2.8b\n\n\n\n\nLoading... (need help?)"
  },
  {
    "objectID": "posts/catalog.html#bigrams-and-trigrams",
    "href": "posts/catalog.html#bigrams-and-trigrams",
    "title": "A catalog of several million tasks Pythia can do",
    "section": "Bigrams and Trigrams",
    "text": "Bigrams and Trigrams\nTo construct bigram and trigram statistics, we process the entire deduplicated Pile.\nWe share six datasets on Huggingface. Descriptions of the datasets are available in the linked dataset cards:\n\npile_bigrams: Raw bigram statistics:\n\n479 million unique bigrams.\n\npile_bigram_prefixes: All bigram prefixes with their most common completion token.\n\n50,054 unique bigram prefixes (one row for each unique token).\n\npile_top_bigrams: Those bigram prefixes for which the most common completion has &gt; 50% probability. We add Pythia’s probability of the most frequent completion for each Pythia model.\n\n3,448 such bigram prefixes. All of these are available to browse on this page above.\n\npile_trigrams: Raw trigram statistics.\n\n9.9 billion unique trigrams.\n\npile_trigram_prefixes: All trigram prefixes with their most common completion token.\n\n479 million unique trigram prefixes (equivalent to bigrams).\n\npile_top_trigrams: Those trigram prefixes for which the most common completion has &gt; 50% probability and where the prefix occurs more than 1000 times in The Pile. We add Pythia’s probability of the most frequent completion for each Pythia model.\n\n1,542,074 such trigram prefixes. The top 25k are available to browse on this page above.\n\n\nBelow, we show the memorization rates for each Pythia model on the pile_top_bigrams and pile_top_trigrams datasets. Since these datasets have been filtered to cases where the most common completion has &gt; 50% probability, we hope to see models predicting the most common completion with high probability. Larger models perform better, but even Pythia-12B is miscalibrated on 20% of the bigrams and 45% of the trigrams when we ask for prediction of \\(p \\geq 0.45\\).\n\n\n\n\n\n\n\n\n\nUsage notes:\n\nBecause the byte-pair encoding tokenizer from GPT-NeoX [3] was trained on The Pile, there are no single tokens in The Pile where the subsequent token is 100% predictable. However, there are many trigrams that are 100% predictable.\nSome whitespace token bigrams will also tokenize as a single token. For example, with the GPT-NeoX tokenizer, \"\\n\\n\\t\\t\" is a token, \"\\t\" is a token and \"\\n\\n\\t\\t\\t\" is also token. It’s important to be aware of this when automatically tokenizing many prompts because almost all concatenated bigrams will tokenize to two tokens but a few whitespace-related bigrams will tokenize to one token. We have not removed these bigrams from the dataset. This white space tokenization is discussed in Appendix F of [3]."
  },
  {
    "objectID": "posts/catalog.html#first-token-deletion",
    "href": "posts/catalog.html#first-token-deletion",
    "title": "A catalog of several million tasks Pythia can do",
    "section": "First token deletion",
    "text": "First token deletion\nIf deleting the first token of a prompt gives a dramatically different output from a language model, then something interpretable may be going on. For example, consider the prompt, \", or common table\". Given this prompt, Pythia-2.8B predicts the most likely next token is \" expression\" with probability 0.37. Next, we provide an additional token of context in the backwards directions with the prompt, \" chloride, or common table\". Then, the model correctly predicts \" salt\" with probability 0.99.\nWe scan through the pre-training corpus \\({t_0,...,t_N}\\) and compare the output of the model on pairs of prompts:\n\n\\(p_0  = [t_i, ... t_{i + n}]\\) is a contiguous \\(n\\)-token prompt from the pre-training corpus.\n\\(p_1  = [t_{i-1}, t_i, ... t_{i + n}]\\) is an \\((n+1)\\)-token prompt where an additional token, \\(t_{i-1}\\) has been added in the backwards direction in the text.\n\nSuppose \\(M(p)\\) is a model than outputs a probability distribution over output tokens. When \\(M(p_1)\\) differs substantially from \\(M(p_0)\\), we capture the two prompts as a “task”. To be more precise, we accept the task if:\n\\[\\mathrm{JSD}(M(p_0), M(p_1)) &gt; 0.5 ~~~~\\mathrm{and}~~~~ \\max_{i} M(p_1)_i &gt; 0.5\\]\nwhere JSD is the Jensen-Shannon Divergence. This criterion means that we focus on tasks for which the addition of \\(t_{i-1}\\) to the prompt has a large influence and results in a confident prediction. Note that the true next token \\(t_{i + n + 1}\\) does not factor into these criteria and therefore the correctness of the model’s predictions does not affect whether we consider the model to be “completing a task”.\nWe share 1,874,497 tasks produced by prompt scanning with Pythia-2.8B for every sliding 5-token prompt in the first 112.5M tokens of the Pile. The dataset is available on Huggingface: pile_scan_4\n\n\n\nn = 4 for this dataset, meaning that we provide an initial 4-token prompt and then add a single token to the beginning of the prompt for the second 5-token prompt.\nfor 1,067,168 tasks, the most likely token is the same for both prompts. Often the model will become much more confident of its initial prediction after seeing the additional token.\nfor 807,329 tasks, the predicted tokens are different.\n\n\n\nScaling this method to the entire Pile would probably result in a several hundred million such tasks."
  },
  {
    "objectID": "posts/catalog.html#github",
    "href": "posts/catalog.html#github",
    "title": "A catalog of several million tasks Pythia can do",
    "section": "GitHub",
    "text": "GitHub\nThe code to reproduce the datasets here is available at: https://github.com/Confirm-Solutions/catalog"
  }
]