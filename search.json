[
  {
    "objectID": "posts/catalog.html",
    "href": "posts/catalog.html",
    "title": "A catalog of several million tasks Pythia can do.",
    "section": "",
    "text": "We’re sharing datasets that we hope will be useful for language model interpretability."
  },
  {
    "objectID": "posts/catalog.html#the-data",
    "href": "posts/catalog.html#the-data",
    "title": "A catalog of several million tasks Pythia can do.",
    "section": "The data",
    "text": "The data\nIn following sections we will give details on the construction and statistics of these datasets. But before continuing, we share some interactive data previews:\n\nDeletion: the first 25000 rows of pile_scan_4.\nBigrams: the entirety of pile_top_bigrams, which contains bigrams with suffix probability greater than 50%\nTrigrams: the first 25000 rows of pile_top_trigrams, which contains trigrams with suffix probability greater than 50% and count greater than 1000.\n\n\nDeletionBigramsTrigrams\n\n\nThe columns of the table below:\n\ntext: two prompts provided. The additional token of backwards context is surrounded by square brackets. The example above would be written \"[_chloride],_or_common_table\".\ntoken_short: the most likely next token predicted by Pythia-2.8B for the four token prompt.\ntoken_long: the most likely next token predicted by Pythia-2.8B for the five token prompt.\np_short: the probability Pythia-2.8B assigns to token_short.\np_long: the probability Pythia-2.8B assigns to token_long.\nJS: the Jensen-Shannon divergence between the model’s output distributions for the four and five token prompts.\n\nNote:\n\nin the table, spaces are replaced with underscores for clarity.\nthere are offensive tokens in the dataset. We have not removed them.\n\n\n\n\n\n\n\n\n\n\ntext\ntoken_short\ntoken_long\np_short\np_long\nJS\n\n\n\n\nLoading... (need help?)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe table below shows bigram completions in The Pile sorted by the frequency of occurence of the prefix token:\n\nsum_count: the number of times the first token of the bigram occurs in The Pile.\nfrac_max: the fraction of first token appearances that are followed by the most common bigram completion. For example, 50.3% of the time the model sees \" need\", the correct next token is \" to\".\np_2.8b: the probability Pythia-2.8B assigns to the most likely completion token when prompted with just the prefix token.\ntoken#: the tokens of the bigram.\n\nNote:\n\nin the table, spaces are replaced with underscores for clarity.\nthere are offensive tokens in the dataset. We have not removed them.\n\n\n\n\n\n\n\n\n\n\ntoken0\ntoken1\nsum_count\nfrac_max\np_2.8b\n\n\n\n\nLoading... (need help?)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe table below shows trigram completions in The Pile sorted by the frequency of occurence of the prefix bigram:\n\nsum_count: the number of times the prefix bigram occurs in The Pile.\nfrac_max: the fraction of bigram appearances that are followed by the most common third token. For example, when prompted with the tokens [\"://\", \"www\"], 99.4% of the time, the next token is \".\".\np_2.8b: the probability Pythia-2.8B assigns to the most likely completion token when prompted with the prefix bigram.\ntoken#: the tokens of the trigram.\n\nNote:\n\nin the table, spaces are replaced with underscores for clarity.\nthere are offensive tokens in the dataset. We have not removed them.\n\n\n\n\n\n\n\n\n\n\ntoken0\ntoken1\ntoken2\nsum_count\nfrac_max\np_2.8b\n\n\n\n\nLoading... (need help?)"
  },
  {
    "objectID": "posts/catalog.html#bigrams-and-trigrams",
    "href": "posts/catalog.html#bigrams-and-trigrams",
    "title": "A catalog of several million tasks Pythia can do.",
    "section": "Bigrams and Trigrams",
    "text": "Bigrams and Trigrams\nTo construct bigram and trigram statistics, we process the entire deduplicated Pile.\nWe share six datasets on Huggingface. Descriptions of the datasets are available in the linked model cards:\n\npile_bigrams: Raw bigram statistics:\n\n479 million unique bigrams.\n\npile_bigram_prefixes: All bigram prefixes with their most common completion token.\n\n50,054 unique bigram prefixes (equivalent to tokens/unigrams).\n\npile_top_bigrams: Those bigram prefixes for which the most common completion has &gt; 50% probability. We add Pythia’s probability of the most frequent completion for each Pythia model.\n\n3,448 such bigram prefixes. All of these are available to browse above.\n\npile_trigrams: Raw trigram statistics.\n\n9.9 billion unique trigrams.\n\npile_trigram_prefixes: All trigram prefixes with their most common completion token.\n\n479 million unique trigram prefixes (equivalent to bigrams).\n\npile_top_trigrams: Those trigram prefixes for which the most common completion has &gt; 50% probability and where the prefix occurs more than 1000 times in The Pile. We add Pythia’s probability of the most frequent completion for each Pythia model.\n\n1,542,074 such trigram prefixes. The top 25k are available to browse above.\n\n\nBelow, we show the memorization rates for each Pythia model on the pile_top_bigrams and pile_top_trigrams datasets. Since these datasets have been filtered to cases where the most common completion has &gt; 50% probability, we hope to see models predicting the most common completion with high probability. Larger models perform better, but even Pythia-12B is miscalibrated on 20% of the bigrams and 45% of the trigrams when we ask for prediction of \\(p \\geq 0.45\\).\n\n\n\n\n\n\n\n\n\nUsage notes:\n\nBecause the byte-pair encoding tokenizer from GPT-NeoX (Black et al. 2022) was trained on The Pile, there are no single tokens in The Pile where the subsequent token is 100% predictable. However, there are many trigrams that are 100% predictable.\nSome whitespace token bigrams will also tokenize as a single token. For example, with the GPT-NeoX tokenizer, \"\\n\\n\\t\\t\" is a token, \"\\t\" is a token and \"\\n\\n\\t\\t\\t\" is also token. It’s important to be aware of this when automatically tokenizing many prompts because almost all concatenated bigrams will tokenize to two tokens but a few whitespace-related bigrams will tokenize to one token. We have not removed these bigrams from the dataset. This white space tokenization is discussed in Appendix F of Black et al. (2022)."
  },
  {
    "objectID": "posts/catalog.html#first-token-deletion",
    "href": "posts/catalog.html#first-token-deletion",
    "title": "A catalog of several million tasks Pythia can do.",
    "section": "First token deletion",
    "text": "First token deletion\nIf deleting the first token of a prompt gives a dramatically different output from a language model, then something interpretable may be going on. For example, consider the prompt, \", or common table\". Given this prompt, Pythia-2.8B predicts the most likely next token is \" expression\" with probability 0.37. Next, we provide an additional token of context in the backwards directions with the prompt, \" chloride, or common table\". Then, the model correctly predicts \" salt\" with probability 0.99.\nWe scan through the pre-training corpus \\({t_0,...,t_N}\\) and compare the output of the model on pairs of prompts:\n\n\\(p_0 = [t_i, ... t_{i + n}]\\) is a contiguous \\(n\\)-token prompt from the pre-training corpus.\n\\(p_1 = [t_{i-1}, t_i, ... t_{i + n}]\\) is an \\((n+1)\\)-token prompt where an additional token, \\(t_{i-1}\\) has been added in the backwards direction in the text.\n\nSuppose \\(M(p)\\) is a model than outputs a probability distribution over output tokens. When \\(M(p_1)\\) differs substantially from \\(M(p_0)\\), we capture the two prompts as a “task”. To be more precise, we accept the task if:\n\\[\\mathrm{JSD}(M(p_0), M(p_1)) &gt; 0.5 ~~~~\\mathrm{and}~~~~ \\max_{i} M(p_1)_i &gt; 0.5\\]\nwhere JSD is the Jensen-Shannon Divergence. This criterion means that we focus on tasks for which the addition of \\(t_{i-1}\\) to the prompt has a large influence and results in a confident prediction. Note that the true next token \\(t_{i + n + 1}\\) does not factor into these criteria and therefore the correctness of the model’s predictions does not affect whether we consider the model to successfully be completing a task.\nWe share 1,874,497 tasks produced by prompt scanning with Pythia-2.8B for every sliding 5-token prompt in the first 112.5M tokens of the Pile. The dataset is available on Huggingface: pile_scan_4\n\n\n\nn = 4 for this dataset, meaning that we provide an initial 4-token prompt and then add a single token to the beginning of the prompt for the second 5-token prompt.\nfor 1,067,168 tasks, the most likely token is the same for both prompts. Often the model will become much more confident of its initial prediction after seeing the additional token.\nfor 807,329 tasks, the predicted tokens are different.\n\n\n\nScaling this method to the entire Pile would probably result in a several hundred million such tasks."
  },
  {
    "objectID": "posts/catalog.html#github",
    "href": "posts/catalog.html#github",
    "title": "A catalog of several million tasks Pythia can do.",
    "section": "GitHub",
    "text": "GitHub\nThe code to reproduce the datasets here is available at: https://github.com/Confirm-Solutions/catalog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Confirm",
    "section": "",
    "text": "A catalog of several million tasks Pythia can do.\n\n\n\n\n\n\nT. Ben Thompson, Michael Sklar\n\n\nJun 25, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  }
]