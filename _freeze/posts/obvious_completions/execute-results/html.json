{
  "hash": "411cd030b3ebe9af0c0ca25abd02c5dc",
  "result": {
    "markdown": "---\ntitle: '[DRAFT] A catalog of several million tasks Pythia can do.'\ndate: 6/12/2023\nformat:\n  html:\n    grid:\n      margin-width: 350px\n  ipynb: default\nbibliography: skeleton.bib\n---\n\n## TODO:\n\n- List lots of facts about the data: number of rows, number of bigrams/trigrams, percent that are highly predictive. a few graphs of the data.\n- Examples of input...\n- link to the huggingface pages.\n\n## Introduction\n\nMuch of mechanistic interpretability either focuses on toy tasks in toy models.\n^[We do not aim to criticize such work, only to contrast. There is clearly\nimmense potential in toy model.] (cite a bunch of examples, transformer\ncircuits, other toy work, sparse transformers, jermyn paper). Other work\nfocuses on small datasets of realistic prompts applied to real-world large language\nmodel. (cite IOI, alpaca das, rome, acdc, greater-than, ...). \n\nTo add a new approach, we would like to scale up interpretability methods to a\nlarge dataset of tasks focused on real models.\n\nTo that end, we're sharing two datasets:\n\n1. a dataset of token-bigram and token-trigram statistics from The Pile (CITE)\n   including tables of one and two token prompts with their most likely\n   completions. \n2. an input scanning dataset that uses the Pythia 2.8B model to determine which\n   tokens are extremely predictive in context.\n   \nThe construction of both datasets is unsupervised. The input scanning dataset\n covers ~100M tokens of The Pile and could be extended to the full dataset\nif desired.\n\n## Bigrams and Trigrams\n\nWe process the entire deduplicated Pile for bigram and trigram counts. [link to the code for a further description of the construction of the dataset]... \n\nSome whitespace token bigrams will also tokenize as a single token. For\nexample, with the GPT-NeoX tokenizer, `\\n\\n\\t\\t` is a token, `\\t` is a token\n*and* `\\n\\n\\t\\t\\t` is also token. This can cause problems when automatically\ntokenizing many prompts.\n\n## Input scanning\n\nAn unsupervised method for finding examples of a model's predictive capabilities.\n\nWe scan through the pre-training corpus ${t_0,...,t_N}$ and compare the output\nof the model on a pair of prompts:\n- $p_0  = [t_i, ... t_{i + n}]$ is a contiguous $n$-token prompt from the\n  pre-training corpus.\n- $p_1  = [t_{i-1}, t_i, ... t_{i + n}]$ is an $(n+1)$-token prompt where an\n  additional token, $t_{i-1}$ has been added in the backwards direction in the\n  text.\n\nWhen $M(p_1)$ differs substantially from $M(p_0)$, we capture the two prompts\nas a \"task\" that the model has succeeded at regardless of whether the model is\ncorrect or not. To be more precise, we accept the task if:\n\n$$\\mathrm{JSD}(M(p_1), M(p_2)) > 0.5$$\n\nwhere JSD is the [Jensen-Shannon Divergence](https://en.wikipedia.org/wiki/Jensenâ€“Shannon_divergence). By focusing on tasks for which the addition of $t_{i - 1}$ to the prompt had a large influence, we expect to find \n\n## What we're using these datasets for?\nThere's a lot we want to do with this data. \n\n- Producing a mechanistic map of a model's internals:\n\t- Mapping out the features and circuits responsible for each task. \n\t- Are there patterns to \n\t- This map will \n- Using the dataset as a benchmark for automated circuit identification\n  algorithms like (ACDC, boundless DAS). \n- Look for inputs that activate portions of the network for which we don't have\n  any activators yet. \n- Finding examples of superposition in the wild. The number of tasks available\n  via input scanning exceeds the number of neurons in Pythia 2.8B (TODO: how\n  many?). Therefore, we should be able to find directions in activation space\n  that interfere.\n\n\n## Likely bigram completions\n\nThe table below shows the bigram completions in The Pile sorted by their frequency of occurence:\n\n- `sum_count`: the number of times the first two tokens of the trigram occur in The Pile.\n- `frac_max`: the fraction of instances where the third token is the completion of the trigram.\n- `token#`: the tokens of the bigram.\n\n<iframe width=\"100%\" height=\"492\" frameborder=\"0\"\n  src=\"https://observablehq.com/embed/@confirmlabs/ngrams?cells=bigrams\"></iframe>\n\n## Likely trigram completions\n\nThe table below shows 100,000 of the most predictive trigrams in The Pile sorted by their frequency of occurence:\n\n- `sum_count`: the number of times the first two tokens of the trigram occur in The Pile.\n- `frac_max`: the fraction of instances where the third token is the completion of the trigram.\n- `token#`: the tokens of the trigram.\n\n<iframe width=\"100%\" height=\"492\" frameborder=\"0\"\n  src=\"https://observablehq.com/embed/@confirmlabs/ngrams?cells=trigrams\"></iframe>\n\n",
    "supporting": [
      "obvious_completions_files"
    ],
    "filters": [],
    "includes": {}
  }
}