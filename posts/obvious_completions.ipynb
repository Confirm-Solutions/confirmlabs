{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \\[DRAFT\\] A catalog of several million tasks Pythia can do.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Much of mechanistic interpretability either focuses on toy tasks in toy\n",
        "models. [1] (cite a bunch of examples, transformer circuits, other toy\n",
        "work, sparse transformers, jermyn paper). Other work focuses on small\n",
        "datasets of realistic prompts applied to real-world large language\n",
        "model. (cite IOI, alpaca das, rome, acdc, greater-than, …).\n",
        "\n",
        "To add a new approach, we would like to scale up interpretability\n",
        "methods to a large dataset of tasks focused on real models.\n",
        "\n",
        "To that end, we’re sharing two datasets:\n",
        "\n",
        "1.  a dataset of token-bigram and token-trigram statistics from The Pile\n",
        "    (CITE) including tables of one and two token prompts with their most\n",
        "    likely completions.\n",
        "2.  an input scanning dataset that uses the Pythia 2.8B model to\n",
        "    determine which tokens are extremely predictive in context.\n",
        "\n",
        "The construction of both datasets is unsupervised. The input scanning\n",
        "dataset covers ~100M tokens of The Pile and could be extended to the\n",
        "full dataset if desired.\n",
        "\n",
        "## Bigrams and Trigrams\n",
        "\n",
        "We process the entire deduplicated Pile for bigram and trigram counts.\n",
        "\\[link to the code for a further description of the construction of the\n",
        "dataset\\]…\n",
        "\n",
        "Some whitespace token bigrams will also tokenize as a single token. For\n",
        "example, with the GPT-NeoX tokenizer, `\\n\\n\\t\\t` is a token, `\\t` is a\n",
        "token *and* `\\n\\n\\t\\t\\t` is also token. This can cause problems when\n",
        "automatically tokenizing many prompts.\n",
        "\n",
        "-   TODO: List lots of facts about the data: number of rows, number of\n",
        "    bigrams/trigrams, percent that are highly predictive. a few graphs\n",
        "    of the data.\n",
        "\n",
        "## Input scanning\n",
        "\n",
        "An unsupervised method for finding examples of a model’s predictive\n",
        "capabilities.\n",
        "\n",
        "We scan through the pre-training corpus ${t_0,...,t_N}$ and compare the\n",
        "output of the model on a pair of prompts: - $p_0 = [t_i, ... t_{i + n}]$\n",
        "is a contiguous $n$-token prompt from the pre-training corpus. -\n",
        "$p_1 = [t_{i-1}, t_i, ... t_{i + n}]$ is an $(n+1)$-token prompt where\n",
        "an additional token, $t_{i-1}$ has been added in the backwards direction\n",
        "in the text.\n",
        "\n",
        "When $M(p_1)$ differs substantially from $M(p_0)$, we capture the two\n",
        "prompts as a “task” that the model has succeeded at regardless of\n",
        "whether the model is correct or not. To be more precise, we accept the\n",
        "task if:\n",
        "\n",
        "$$\\mathrm{JSD}(M(p_1), M(p_2)) > 0.5$$\n",
        "\n",
        "where JSD is the [Jensen-Shannon\n",
        "Divergence](https://en.wikipedia.org/wiki/Jensen–Shannon_divergence). By\n",
        "focusing on tasks for which the addition of $t_{i - 1}$ to the prompt\n",
        "had a large influence, we expect to find\n",
        "\n",
        "-   TODO: List lots of facts about the data: number of rows, number of\n",
        "    bigrams/trigrams, percent that are highly predictive. a few graphs\n",
        "    of the data.\n",
        "\n",
        "TODO: Examples of input…\n",
        "\n",
        "TODO: link to the huggingface pages.\n",
        "\n",
        "## What we’re using these datasets for?\n",
        "\n",
        "There’s a lot we want to do with this data.\n",
        "\n",
        "-   Producing a mechanistic map of a model’s internals:\n",
        "    -   Mapping out the features and circuits responsible for each task.\n",
        "    -   Are there patterns to\n",
        "    -   This map will\n",
        "-   Using the dataset as a benchmark for automated circuit\n",
        "    identification algorithms like (ACDC, boundless DAS).\n",
        "-   Look for inputs that activate portions of the network for which we\n",
        "    don’t have any activators yet.\n",
        "-   Finding examples of superposition in the wild. The number of tasks\n",
        "    available via input scanning exceeds the number of neurons in Pythia\n",
        "    2.8B (TODO: how many?). Therefore, we should be able to find\n",
        "    directions in activation space that interfere.\n",
        "\n",
        "## Likely bigram completions\n",
        "\n",
        "The table below shows the bigram completions in The Pile sorted by their\n",
        "frequency of occurence: - `sum_count`: the number of times the first two\n",
        "tokens of the trigram occur in The Pile. - `frac_max`: the fraction of\n",
        "instances where the third token is the completion of the trigram. -\n",
        "`token#`: the tokens of the bigram.\n",
        "\n",
        "[1] We do not aim to criticize such work, only to contrast. There is\n",
        "clearly immense potential in toy model."
      ],
      "id": "fc1f7887-f5f6-4548-b41d-41c59e0b52a0"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<iframe width=\"100%\" height=\"492\" frameborder=\"0\" src=\"https://observablehq.com/embed/@confirmlabs/ngrams?cells=bigrams\">"
      ],
      "id": "893fda49-6bc6-406c-9fe3-32809d2f9c80"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</iframe>"
      ],
      "id": "3efedd60-7052-4038-817f-09b1278da87e"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Likely trigram completions\n",
        "\n",
        "The table below shows 100,000 of the most predictive trigrams in The\n",
        "Pile sorted by their frequency of occurence: - `sum_count`: the number\n",
        "of times the first two tokens of the trigram occur in The Pile. -\n",
        "`frac_max`: the fraction of instances where the third token is the\n",
        "completion of the trigram. - `token#`: the tokens of the trigram."
      ],
      "id": "20c01c4c-eccf-431b-aaec-fe704ae09506"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "<iframe width=\"100%\" height=\"492\" frameborder=\"0\" src=\"https://observablehq.com/embed/@confirmlabs/ngrams?cells=trigrams\">"
      ],
      "id": "d3df17ad-5319-4702-953e-415325d70f3e"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "raw_mimetype": "text/html"
      },
      "source": [
        "</iframe>"
      ],
      "id": "b064c852-c813-44db-900c-983ebf0beb60"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "codemirror_mode": {
        "name": "ipython",
        "version": "3"
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  }
}