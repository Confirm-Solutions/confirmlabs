{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \\[DRAFT\\] A few million tasks that Pythia can do\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Much of mechanistic interpretability either:\n",
        "\n",
        "1.  Focuses on toy tasks in toy models. (footnote: not a bad thing)\n",
        "    (cite a bunch of examples, transformer circuits, other toy work,\n",
        "    sparse transformers, jermyn paper)\n",
        "2.  Focuses on a small dataset of realistic prompts applied to a real\n",
        "    model. (IOI, alpaca das, rome, acdc, greater-than, …).\n",
        "\n",
        "To add a new approach, we would like to scale up interpretability\n",
        "methods to a large dataset of tasks focused on real models.\n",
        "\n",
        "To that end, we’re sharing two datasets:\n",
        "\n",
        "1.  a dataset of token-bigram and token-trigram statistics from The Pile\n",
        "    (CITE) including tables of one and two token prompts with their most\n",
        "    likely completions.\n",
        "2.  an input scanning dataset that uses the Pythia 1.4B model to\n",
        "    determine what tokens are extremely predictive in context.\n",
        "\n",
        "### Bigrams and Trigrams\n",
        "\n",
        "We process the entire deduplicated Pile for bigram and trigram counts.\n",
        "\\[link to the code for a further description of the construction of the\n",
        "dataset\\]…\n",
        "\n",
        "### Input scanning\n",
        "\n",
        "An unsupervised method for identifying millions of different things that\n",
        "a model knows.\n",
        "\n",
        "We scan through the pre-training corpus ${t_0,...,t_N}$ and compare the\n",
        "output of the model on a pair of prompts: - $p_0 = [t_i, ... t_{i + n}]$\n",
        "is a contiguous $n$-token prompt from the pre-training corpus. -\n",
        "$p_1 = [t_{i-1}, t_i, ... t_{i + n}]$ is an $(n+1)$-token prompt where\n",
        "an additional token, $t_{i-1}$ has been added in the backwards direction\n",
        "in the text. When $M(p_1)$ differs substantially from $M(p_0)$, we\n",
        "capture the two prompts as a “task” that the model has succeeded at\n",
        "regardless of whether the model is correct or not. To be more precise,\n",
        "we accept the task if: $$\\mathrm{JSD}(M(p_1), M(p_2)) > 0.5$$ where JSD\n",
        "is the [Jensen-Shannon\n",
        "Divergence](https://en.wikipedia.org/wiki/Jensen–Shannon_divergence). By\n",
        "focusing on tasks for which the addition of $t_{i - 1}$ to the prompt\n",
        "had a large influence, we expect to find\n",
        "\n",
        "Examples of input\n",
        "\n",
        "### What we’re using these datasets for?\n",
        "\n",
        "There’s a lot we want to do with this data. - Producing a mechanistic\n",
        "map of a model’s internals: - Mapping out the features and circuits\n",
        "responsible for each task. - Are there patterns to - This map will -\n",
        "Using the dataset as a benchmark for automated circuit identification\n",
        "algorithms like (ACDC, boundless DAS). - Look for inputs that activate\n",
        "portions of the network for which we don’t have any activators yet. -\n",
        "Finding examples of superposition in the wild. The number of tasks\n",
        "available via input scanning exceeds the number of multilayer perceptron\n",
        "neurons in Pythia 2.8B. Therefore, we may be able to find directions in\n",
        "activation space that interfere.\n",
        "\n",
        "### TODO:\n",
        "\n",
        "-   List lots of facts about the data: number of rows, number of\n",
        "    bigrams/trigrams, percent that are highly predictive. graphs of the\n",
        "    data.\n",
        "-   Present a page-able interactive data viewer.\n",
        "-   warn about the token bigrams that tokenize as a single token."
      ],
      "id": "c9f4434b-35bf-43f2-8f42-576ce96edb91"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {}
        },
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {}
        },
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {}
        }
      ],
      "source": [],
      "id": "6f6f8c8c-61df-4c3e-85c9-ae936d4b5d34"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {}
        },
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {}
        }
      ],
      "source": [],
      "id": "a17d4deb-e890-4202-92eb-04d7dbedaea6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "output_type": "display_data",
          "metadata": {},
          "data": {}
        }
      ],
      "source": [],
      "id": "5857a12c-67ba-460d-99ac-63651364a157"
    }
  ],
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3 (ipykernel)",
      "language": "python"
    }
  }
}