@inproceedings{black-etal-2022-gpt,
    title = "{GPT}-{N}eo{X}-20{B}: An Open-Source Autoregressive Language Model",
    author = "Black, Sidney  and
      Biderman, Stella  and
      Hallahan, Eric  and
      Anthony, Quentin  and
      Gao, Leo  and
      Golding, Laurence  and
      He, Horace  and
      Leahy, Connor  and
      McDonell, Kyle  and
      Phang, Jason  and
      Pieler, Michael  and
      Prashanth, Usvsn Sai  and
      Purohit, Shivanshu  and
      Reynolds, Laria  and
      Tow, Jonathan  and
      Wang, Ben  and
      Weinbach, Samuel",
    booktitle = "Proceedings of BigScience Episode {\#}5 -- Workshop on Challenges {\&} Perspectives in Creating Large Language Models",
    month = may,
    year = "2022",
    address = "virtual+Dublin",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.bigscience-1.9",
    doi = "10.18653/v1/2022.bigscience-1.9",
    pages = "95--136",
    abstract = "We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights at the time of submission. In this work, we describe GPT-NeoX-20B{'}s architecture and training, and evaluate its performance. We open-source the training and evaluation code, as well as the model weights, at https://github.com/EleutherAI/gpt-neox.",
}

@article{pile,
  title={The {P}ile: An 800GB Dataset of Diverse Text for Language Modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@misc{biderman2023pythia,
      title={Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling}, 
      author={Stella Biderman and Hailey Schoelkopf and Quentin Anthony and Herbie Bradley and Kyle O'Brien and Eric Hallahan and Mohammad Aflah Khan and Shivanshu Purohit and USVSN Sai Prashanth and Edward Raff and Aviya Skowron and Lintang Sutawika and Oskar van der Wal},
      year={2023},
      eprint={2304.01373},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{elhage2021mathematical,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}

@article{olah2017feature,
      author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
      title = {Feature Visualization},
      journal = {Distill},
      year = {2017},
      note = {https://distill.pub/2017/feature-visualization},
      doi = {10.23915/distill.00007}
}

@misc{bauerle-2018,
    	author = {Bäuerle, Alex and  Wexler, James},
    	title = {{What does BERT dream of?}},
    	year = {2018},
    	url = {https://pair-code.github.io/interpretability/text-dream/blogpost/},
}

@misc{mordvintsev-2015,
      title	= {Inceptionism: Going Deeper into Neural Networks},
      author	= {Alexander Mordvintsev and Christopher Olah and Mike Tyka},
      year	= {2015},
      URL	= {https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html}
}

@misc{jang2017categorical,
      title={Categorical Reparameterization with Gumbel-Softmax}, 
      author={Eric Jang and Shixiang Gu and Ben Poole},
      year={2017},
      eprint={1611.01144},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{zou2023universal,
      title={Universal and Transferable Adversarial Attacks on Aligned Language Models}, 
      author={Andy Zou and Zifan Wang and J. Zico Kolter and Matt Fredrikson},
      year={2023},
      eprint={2307.15043},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wen2023hard,
      title={Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery}, 
      author={Yuxin Wen and Neel Jain and John Kirchenbauer and Micah Goldblum and Jonas Geiping and Tom Goldstein},
      year={2023},
      eprint={2302.03668},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{guo2021gradientbased,
      title={Gradient-based Adversarial Attacks against Text Transformers}, 
      author={Chuan Guo and Alexandre Sablayrolles and Hervé Jégou and Douwe Kiela},
      year={2021},
      eprint={2104.13733},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wallace2021universal,
      title={Universal Adversarial Triggers for Attacking and Analyzing NLP}, 
      author={Eric Wallace and Shi Feng and Nikhil Kandpal and Matt Gardner and Sameer Singh},
      year={2021},
      eprint={1908.07125},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{kumar2022gradientbased,
      title={Gradient-Based Constrained Sampling from Language Models}, 
      author={Sachin Kumar and Biswajit Paria and Yulia Tsvetkov},
      year={2022},
      eprint={2205.12558},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{shin2020autoprompt,
      title={AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts}, 
      author={Taylor Shin and Yasaman Razeghi and Robert L. Logan IV au2 and Eric Wallace and Sameer Singh},
      year={2020},
      eprint={2010.15980},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{shi2022human,
      title={Toward Human Readable Prompt Tuning: Kubrick's The Shining is a good movie, and a good prompt too?}, 
      author={Weijia Shi and Xiaochuang Han and Hila Gonen and Ari Holtzman and Yulia Tsvetkov and Luke Zettlemoyer},
      year={2022},
      eprint={2212.10539},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ebrahimi2018hotflip,
      title={HotFlip: White-Box Adversarial Examples for Text Classification}, 
      author={Javid Ebrahimi and Anyi Rao and Daniel Lowd and Dejing Dou},
      year={2018},
      eprint={1712.06751},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{gurnee2023finding,
      title={Finding Neurons in a Haystack: Case Studies with Sparse Probing}, 
      author={Wes Gurnee and Neel Nanda and Matthew Pauly and Katherine Harvey and Dmitrii Troitskii and Dimitris Bertsimas},
      year={2023},
      eprint={2305.01610},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{elhage2022superposition,
   title={Toy Models of Superposition},
   author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and Grosse, Roger and McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Wattenberg, Martin and Olah, Christopher},
   year={2022},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2022/toy_model/index.html}
}

@misc{yuan2023bridge,
      title={Bridge the Gap Between CV and NLP! A Gradient-based Textual Adversarial Attack Framework}, 
      author={Lifan Yuan and Yichi Zhang and Yangyi Chen and Wei Wei},
      year={2023},
      eprint={2110.15317},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{jones2023automatically,
      title={Automatically Auditing Large Language Models via Discrete Optimization}, 
      author={Erik Jones and Anca Dragan and Aditi Raghunathan and Jacob Steinhardt},
      year={2023},
      eprint={2303.04381},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

 @article{bricken2023monosemanticity,
       title={Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
       author={Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Hatfield-Dodds, Zac and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Christopher},
       year={2023},
       journal={Transformer Circuits Thread},
       note={https://transformer-circuits.pub/2023/monosemantic-features/index.html}
    }


@misc{bills2023language,
 title={Language models can explain neurons in language models},
 author={
    Bills, Steven and Cammarata, Nick and Mossing, Dan and Tillman, Henk and Gao, Leo and Goh, Gabriel and Sutskever, Ilya and Leike, Jan and Wu, Jeff and Saunders, William
 },
 year={2023},
 howpublished = {\url{https://openaipublic.blob.core.windows.net/neuron-explainer/paper/index.html}}
}


@misc{bolukbasi2021interpretability,
      title={An Interpretability Illusion for BERT}, 
      author={Tolga Bolukbasi and Adam Pearce and Ann Yuan and Andy Coenen and Emily Reif and Fernanda Viégas and Martin Wattenberg},
      year={2021},
      eprint={2104.07143},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{yosinski2015understanding,
      title={Understanding Neural Networks Through Deep Visualization}, 
      author={Jason Yosinski and Jeff Clune and Anh Nguyen and Thomas Fuchs and Hod Lipson},
      year={2015},
      eprint={1506.06579},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{szegedy2014intriguing,
      title={Intriguing properties of neural networks}, 
      author={Christian Szegedy and Wojciech Zaremba and Ilya Sutskever and Joan Bruna and Dumitru Erhan and Ian Goodfellow and Rob Fergus},
      year={2014},
      eprint={1312.6199},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@misc{cunningham2023sparse,
      title={Sparse Autoencoders Find Highly Interpretable Features in Language Models}, 
      author={Hoagy Cunningham and Aidan Ewart and Logan Riggs and Robert Huben and Lee Sharkey},
      year={2023},
      eprint={2309.08600},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{cammarata2020thread,
  author = {Cammarata, Nick and Carter, Shan and Goh, Gabriel and Olah, Chris and Petrov, Michael and Schubert, Ludwig and Voss, Chelsea and Egan, Ben and Lim, Swee Kiat},
  title = {Thread: Circuits},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/circuits},
  doi = {10.23915/distill.00024}
}

@article{sadasivan2024fast,
  title={Fast Adversarial Attacks on Language Models In One GPU Minute},
  author={Sadasivan, Vinu Sankar and Saha, Shoumik and Sriramanan, Gaurang and Kattakinda, Priyatham and Chegini, Atoosa and Feizi, Soheil},
  journal={arXiv preprint arXiv:2402.15570},
  year={2024}
}

@misc{zou2024improvingalignmentrobustnesscircuit,
      title={Improving Alignment and Robustness with Circuit Breakers}, 
      author={Andy Zou and Long Phan and Justin Wang and Derek Duenas and Maxwell Lin and Maksym Andriushchenko and Rowan Wang and Zico Kolter and Matt Fredrikson and Dan Hendrycks},
      year={2024},
      eprint={2406.04313},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2406.04313}, 
}

@misc{cui2024orbenchoverrefusalbenchmarklarge,
      title={OR-Bench: An Over-Refusal Benchmark for Large Language Models}, 
      author={Justin Cui and Wei-Lin Chiang and Ion Stoica and Cho-Jui Hsieh},
      year={2024},
      eprint={2405.20947},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.20947}, 
}

@misc{bloom2024gpt2residualsaes,
   title = {Open Source Sparse Autoencoders for all Residual Stream Layers of GPT2 Small},
   author = {Joseph Bloom},
   year = {2024},
   howpublished = {\url{https://www.alignmentforum.org/posts/f9EgfLSurAiqRJySD/open-source-sparse-autoencoders-for-all-residual-stream}},
}

@misc{lieberum2024gemmascopeopensparse,
      title={Gemma Scope: Open Sparse Autoencoders Everywhere All At Once on Gemma 2}, 
      author={Tom Lieberum and Senthooran Rajamanoharan and Arthur Conmy and Lewis Smith and Nicolas Sonnerat and Vikrant Varma and János Kramár and Anca Dragan and Rohin Shah and Neel Nanda},
      year={2024},
      eprint={2408.05147},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2408.05147}, 
}

@misc{thompson2024flrtfluentstudentteacherredteaming,
      title={FLRT: Fluent Student-Teacher Redteaming}, 
      author={T. Ben Thompson and Michael Sklar},
      year={2024},
      eprint={2407.17447},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2407.17447}, 
}