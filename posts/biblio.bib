@inproceedings{black-etal-2022-gpt,
    title = "{GPT}-{N}eo{X}-20{B}: An Open-Source Autoregressive Language Model",
    author = "Black, Sidney  and
      Biderman, Stella  and
      Hallahan, Eric  and
      Anthony, Quentin  and
      Gao, Leo  and
      Golding, Laurence  and
      He, Horace  and
      Leahy, Connor  and
      McDonell, Kyle  and
      Phang, Jason  and
      Pieler, Michael  and
      Prashanth, Usvsn Sai  and
      Purohit, Shivanshu  and
      Reynolds, Laria  and
      Tow, Jonathan  and
      Wang, Ben  and
      Weinbach, Samuel",
    booktitle = "Proceedings of BigScience Episode {\#}5 -- Workshop on Challenges {\&} Perspectives in Creating Large Language Models",
    month = may,
    year = "2022",
    address = "virtual+Dublin",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.bigscience-1.9",
    doi = "10.18653/v1/2022.bigscience-1.9",
    pages = "95--136",
    abstract = "We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights at the time of submission. In this work, we describe GPT-NeoX-20B{'}s architecture and training, and evaluate its performance. We open-source the training and evaluation code, as well as the model weights, at https://github.com/EleutherAI/gpt-neox.",
}

@article{pile,
  title={The {P}ile: An 800GB Dataset of Diverse Text for Language Modeling},
  author={Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
  journal={arXiv preprint arXiv:2101.00027},
  year={2020}
}

@misc{biderman2023pythia,
      title={Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling}, 
      author={Stella Biderman and Hailey Schoelkopf and Quentin Anthony and Herbie Bradley and Kyle O'Brien and Eric Hallahan and Mohammad Aflah Khan and Shivanshu Purohit and USVSN Sai Prashanth and Edward Raff and Aviya Skowron and Lintang Sutawika and Oskar van der Wal},
      year={2023},
      eprint={2304.01373},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@article{elhage2021mathematical,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}

@article{olah2017feature,
      author = {Olah, Chris and Mordvintsev, Alexander and Schubert, Ludwig},
      title = {Feature Visualization},
      journal = {Distill},
      year = {2017},
      note = {https://distill.pub/2017/feature-visualization},
      doi = {10.23915/distill.00007}
}

@misc{bauerle-2018,
    	author = {Bäuerle, Alex and  Wexler, James},
    	title = {{What does BERT dream of?}},
    	year = {2018},
    	url = {https://pair-code.github.io/interpretability/text-dream/blogpost/},
}

@misc{mordvintsev-2015,
      title	= {Inceptionism: Going Deeper into Neural Networks},
      author	= {Alexander Mordvintsev and Christopher Olah and Mike Tyka},
      year	= {2015},
      URL	= {https://research.googleblog.com/2015/06/inceptionism-going-deeper-into-neural.html}
}

@misc{jang2017categorical,
      title={Categorical Reparameterization with Gumbel-Softmax}, 
      author={Eric Jang and Shixiang Gu and Ben Poole},
      year={2017},
      eprint={1611.01144},
      archivePrefix={arXiv},
      primaryClass={stat.ML}
}

@misc{zou2023universal,
      title={Universal and Transferable Adversarial Attacks on Aligned Language Models}, 
      author={Andy Zou and Zifan Wang and J. Zico Kolter and Matt Fredrikson},
      year={2023},
      eprint={2307.15043},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wen2023hard,
      title={Hard Prompts Made Easy: Gradient-Based Discrete Optimization for Prompt Tuning and Discovery}, 
      author={Yuxin Wen and Neel Jain and John Kirchenbauer and Micah Goldblum and Jonas Geiping and Tom Goldstein},
      year={2023},
      eprint={2302.03668},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@misc{guo2021gradientbased,
      title={Gradient-based Adversarial Attacks against Text Transformers}, 
      author={Chuan Guo and Alexandre Sablayrolles and Hervé Jégou and Douwe Kiela},
      year={2021},
      eprint={2104.13733},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{wallace2021universal,
      title={Universal Adversarial Triggers for Attacking and Analyzing NLP}, 
      author={Eric Wallace and Shi Feng and Nikhil Kandpal and Matt Gardner and Sameer Singh},
      year={2021},
      eprint={1908.07125},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{kumar2022gradientbased,
      title={Gradient-Based Constrained Sampling from Language Models}, 
      author={Sachin Kumar and Biswajit Paria and Yulia Tsvetkov},
      year={2022},
      eprint={2205.12558},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{shin2020autoprompt,
      title={AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts}, 
      author={Taylor Shin and Yasaman Razeghi and Robert L. Logan IV au2 and Eric Wallace and Sameer Singh},
      year={2020},
      eprint={2010.15980},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{shi2022human,
      title={Toward Human Readable Prompt Tuning: Kubrick's The Shining is a good movie, and a good prompt too?}, 
      author={Weijia Shi and Xiaochuang Han and Hila Gonen and Ari Holtzman and Yulia Tsvetkov and Luke Zettlemoyer},
      year={2022},
      eprint={2212.10539},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{ebrahimi2018hotflip,
      title={HotFlip: White-Box Adversarial Examples for Text Classification}, 
      author={Javid Ebrahimi and Anyi Rao and Daniel Lowd and Dejing Dou},
      year={2018},
      eprint={1712.06751},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{gurnee2023finding,
      title={Finding Neurons in a Haystack: Case Studies with Sparse Probing}, 
      author={Wes Gurnee and Neel Nanda and Matthew Pauly and Katherine Harvey and Dmitrii Troitskii and Dimitris Bertsimas},
      year={2023},
      eprint={2305.01610},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{elhage2022superposition,
   title={Toy Models of Superposition},
   author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and Grosse, Roger and McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Wattenberg, Martin and Olah, Christopher},
   year={2022},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2022/toy_model/index.html}
}