---
title: '[DRAFT] A catalog of several million tasks Pythia can do.'
date: 6/12/2023
format:
  html:
    grid:
      margin-width: 350px
  ipynb: default
bibliography: skeleton.bib
jupyter: python3
execute:
  echo: false
---

## Introduction

Much of mechanistic interpretability either focuses on toy tasks in toy models.
^[We do not aim to criticize such work, only to contrast. There is clearly
immense potential in toy model.] (cite a bunch of examples, transformer
circuits, other toy work, sparse transformers, jermyn paper). Other work
focuses on small datasets of realistic prompts applied to real-world large language
model. (cite IOI, alpaca das, rome, acdc, greater-than, ...). 

To add a new approach, we would like to scale up interpretability methods to a
large dataset of tasks focused on real models.

To that end, we're sharing two datasets:

1. a dataset of token-bigram and token-trigram statistics from The Pile (CITE)
   including tables of one and two token prompts with their most likely
   completions. 
2. an input scanning dataset that uses the Pythia 2.8B model to determine which
   tokens are extremely predictive in context.
   
The construction of both datasets is unsupervised. The input scanning dataset
 covers ~100M tokens of The Pile and could be extended to the full dataset
if desired.

## Bigrams and Trigrams

We process the entire deduplicated Pile for bigram and trigram counts. [link to the code for a further description of the construction of the dataset]... 

Some whitespace token bigrams will also tokenize as a single token. For
example, with the GPT-NeoX tokenizer, `\n\n\t\t` is a token, `\t` is a token
*and* `\n\n\t\t\t` is also token. This can cause problems when automatically
tokenizing many prompts.

- TODO: List lots of facts about the data: number of rows, number of bigrams/trigrams, percent that are highly predictive. a few graphs of the data.

## Input scanning

An unsupervised method for finding examples of a model's predictive capabilities.

We scan through the pre-training corpus ${t_0,...,t_N}$ and compare the output
of the model on a pair of prompts:
- $p_0  = [t_i, ... t_{i + n}]$ is a contiguous $n$-token prompt from the
  pre-training corpus.
- $p_1  = [t_{i-1}, t_i, ... t_{i + n}]$ is an $(n+1)$-token prompt where an
  additional token, $t_{i-1}$ has been added in the backwards direction in the
  text.

When $M(p_1)$ differs substantially from $M(p_0)$, we capture the two prompts
as a "task" that the model has succeeded at regardless of whether the model is
correct or not. To be more precise, we accept the task if:

$$\mathrm{JSD}(M(p_1), M(p_2)) > 0.5$$

where JSD is the [Jensen-Shannon Divergence](https://en.wikipedia.org/wiki/Jensenâ€“Shannon_divergence). By focusing on tasks for which the addition of $t_{i - 1}$ to the prompt had a large influence, we expect to find 

- TODO: List lots of facts about the data: number of rows, number of bigrams/trigrams, percent that are highly predictive. a few graphs of the data.

TODO: Examples of input...

TODO: link to the huggingface pages.

## What we're using these datasets for?
There's a lot we want to do with this data. 

- Producing a mechanistic map of a model's internals:
	- Mapping out the features and circuits responsible for each task. 
	- Are there patterns to 
	- This map will 
- Using the dataset as a benchmark for automated circuit identification
  algorithms like (ACDC, boundless DAS). 
- Look for inputs that activate portions of the network for which we don't have
  any activators yet. 
- Finding examples of superposition in the wild. The number of tasks available
  via input scanning exceeds the number of neurons in Pythia 2.8B (TODO: how
  many?). Therefore, we should be able to find directions in activation space
  that interfere.


## Likely bigram completions

The table below shows the bigram completions in The Pile sorted by their frequency of occurence:
- `sum_count`: the number of times the first two tokens of the trigram occur in The Pile.
- `frac_max`: the fraction of instances where the third token is the completion of the trigram.
- `token#`: the tokens of the bigram.

<iframe width="100%" height="492" frameborder="0"
  src="https://observablehq.com/embed/@confirmlabs/ngrams?cells=bigrams"></iframe>

## Likely trigram completions

The table below shows 100,000 of the most predictive trigrams in The Pile sorted by their frequency of occurence:
- `sum_count`: the number of times the first two tokens of the trigram occur in The Pile.
- `frac_max`: the fraction of instances where the third token is the completion of the trigram.
- `token#`: the tokens of the trigram.

<iframe width="100%" height="492" frameborder="0"
  src="https://observablehq.com/embed/@confirmlabs/ngrams?cells=trigrams"></iframe>